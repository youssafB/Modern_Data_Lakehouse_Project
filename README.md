# ğŸš² Bike Data Lakehouse: End-to-End Medallion Architecture

## ğŸ“Œ Executive Summary

I built a data lakehouse on Databricks that transforms raw ERP and CRM data into clean, analytics-ready tables. The project uses the Medallion Architecture ( Bronze â†’ Silver â†’ Gold layers) with Delta Lake, PySpark, and Unity Catalog to handle data quality, governance, and historical tracking.

**Tech:** Databricks, Delta Lake, PySpark,SQL, Python, Unity Catalog, Databricks Workflows

---

## ğŸ—ï¸ System Architecture
The pipeline follows a multi-layered approach to ensure data integrity and scalability:

* **ğŸ¥‰Bronze (Raw/Ingestion):** Direct ingestion of source CSVs into Delta tables (Schema-on-read, maintaining original state).
* **ğŸ¥ˆ Silver (Curated/Cleaned):** Data deduplication, standardization of date formats, and business logic normalization (**PySpark/SQL**).
* **ğŸ¥‡Gold (Analytical/Aggregated):** Dimensional modeling (**Star Schema**) optimized for BI tools (PowerBI/Tableau) and advanced analytics.

![Data Lakehouse Architecture](docs/data_Lakehouse.drawio%20(1).svg)

---

## ğŸ› ï¸ Key Features
* **Data Governance:** Implemented **Unity Catalog** for fine-grained access control and metadata management.
* **Orchestration:** Automated end-to-end workflows using **Databricks Workflows (Jobs)** with modular task dependencies.
* **Data Quality:** Systematic cleaning including handling nulls, string normalization, and referential integrity checks.
* **Scalability:** Used a dictionary-driven ingestion pattern to reduce code redundancy (**DRY principle**).

---



## ğŸ” Data Modeling & Intelligence (Gold Layer)
The final layer transitions from source-aligned data to a **Star Schema** (Dimensional Model). This design decouples the analytics layer from the source system logic, providing a high-performance environment for Business Intelligence and Data Science.

However, before building a new data model, we must first understand the original data model:
* What are the main business objects?
* How are these objects related to each other?
* 
By clearly analyzing the initial data model, we can identify three primary domains for the new dimensional model:
* Sales
* Product
* Customer

These domains will serve as the core labels (or subject areas) for designing the new star schema.
![Data Lakehouse Architecture](docs/integration_model.png)


Based on this understanding of the source model and business domains, we designed the Gold Layer using a dimensional modeling approach. The resulting model consists of:

* **Fact Tables:** `fact_sales` (Grain: Individual Transaction). Captures business events and quantitative metrics.
* **Dimension Tables:** `dim_customers`, `dim_products`. Provides descriptive context for slicing and dicing metrics.


![Data Lakehouse Architecture](docs/data_model.PNG)


---


## Data Flow / Lineage

The diagram below shows the data lineage for our data Lakehouse. It helps to understand where the data comes from and makes it easier to troubleshoot issues.

![Data Lakehouse Architecture](docs/data_flow.png)

--- 

## ğŸ“‚ Project Structure
```bach
.
â”œâ”€â”€ datasets/
â”œâ”€â”€ code/
â”‚   â”œâ”€â”€ init_lakehouse.ipynb
â”‚   â”œâ”€â”€ bronze/
â”‚   â”‚   â”œâ”€â”€ bronze.ipynb
â”‚   â”‚   â””â”€â”€ bronze_config.py
â”‚   â”œâ”€â”€ silver/
â”‚   â”‚   â”œâ”€â”€ crm/
â”‚   â”‚   â”‚   â”œâ”€â”€ silver_crm_cust_info.ipynb
â”‚   â”‚   â”‚   â”œâ”€â”€ silver_crm_prd_info.ipynb
â”‚   â”‚   â”‚   â””â”€â”€ silver_crm_sales_details.ipynb
â”‚   â”‚   â”œâ”€â”€ erp/
â”‚   â”‚   â”‚   â”œâ”€â”€ silver_erp_cust_az12.ipynb
â”‚   â”‚   â”‚   â”œâ”€â”€ silver_erp_loc_a101.ipynb
â”‚   â”‚   â”‚   â””â”€â”€ silver_erp_px_cat_g1v2.ipynb
â”‚   â”‚   â””â”€â”€ silver_orchestration.ipynb
â”‚   â””â”€â”€ gold/
â”‚       â”œâ”€â”€ gold_dim_customers.ipynb
â”‚       â”œâ”€â”€ gold_dim_products.ipynb
â”‚       â”œâ”€â”€ gold_fact_sales.ipynb
â”‚       â””â”€â”€ gold_orchestration.ipynb
â”œâ”€â”€ doc/
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md


---
## ğŸ› ï¸ Technologies & Tools

| Component | Technology |
|-----------|-----------|
| **Platform** | Databricks (AWS/Azure) |
| **Storage** | Delta Lake (ACID transactions) |
| **Processing** | Apache Spark (PySpark) |
| **Orchestration** | Databricks Jobs |
| **Catalog** | Unity Catalog |
| **Version Control** | Git (GitHub integration) |
| **Data Modeling** | Star Schema |


---


## ğŸ“ˆ Future Improvements
- [ ] **CI/CD Integration:** Automate deployment using **GitHub Actions** and **Databricks Asset Bundles (DABs)** for a true production-grade DevOps lifecycle.
- [ ] **New data sources** APIs, Kafka, streaming data, and operational databases
- [ ] - **Security and governance** : Access control, row-level security, and data masking


## ğŸ‘¨â€ğŸ’» Author
[Youssef Bouraha] Data Scientist | Data Engineer 
